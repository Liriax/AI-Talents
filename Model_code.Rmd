---
title: "Tech Skillytics"
author: "T(AI)lents"
date: "20/12/2020"
output:
  pdf_document:
    dev: png
  keep_tex: yes
  word_document: default
header-includes:
- \usepackage{dcolumn}
- \usepackage{booktabs}
- \usepackage{sectsty} \sectionfont{\centering}
- \usepackage{indentfirst}
- \usepackage{setspace}\onehalfspacing
fontsize: 11pt


---


```{r setup, include=FALSE, message=FALSE}

knitr::opts_chunk$set(echo = T, cache = TRUE)
options(scipen = 99999) # Do not wish to have scientific  notations.


```


```{r, warning = F, message = F}
# loading the required packages:

library(FactoMineR)
library(tibble)
library(tm)
library(tidyverse)
library(parallel)
library(ggplot2)
library(tidytext)
require(sentimentr)
library(SentimentAnalysis)
library(lexicon)
require(tokenizers)
require(stringr)

```



\pagebreak

\tableofcontents

\pagebreak


----

### 1. Cleaning up the edx data: 

----

```{r}

# Cleaning the edx data----

DF_edx <- read.csv("edx_courses.csv") %>% 
  select(title, course_description , course_length , course_effort) %>% 
  rename( length_in_weeks = course_length )

DF_edx$course_description <- DF_edx$course_description %>% 
  tolower() %>%
  gsub("[^[:alnum:]|\\+]", " ", .) %>% 
  gsub('\\b\\w{21,}\\s','', .) %>% 
  removeWords(., stopwords(kind = "en")) %>% 
  gsub("[[:space:]]+", " ", .)

DF_edx$title <- DF_edx$title %>% 
  tolower() %>%
  gsub("[^[:alnum:]|\\+]", " ", .) %>% 
  gsub('\\b\\w{21,}\\s','', .) %>% 
  removeWords(., stopwords(kind = "en")) %>% 
  gsub("[[:space:]]+", " ", .)

DF_edx$length_in_weeks <- DF_edx$length_in_weeks  %>% 
  gsub(" Weeks", "", .) %>% 
  as.numeric()

DF_edx <- DF_edx %>% 
  mutate(min_hours_per_week = as.numeric(gsub("â€“.+", "", course_effort)),
         max_hours_per_week = as.numeric(gsub(".+â€“| hours per week", "", course_effort)), 
         avg_hours = (min_hours_per_week +max_hours_per_week)/2,
         expected_hours = avg_hours*length_in_weeks) %>%
  select(title, course_description , expected_hours)



```


----

### 2. Checking the skills present in the edx data

----


```{r}

# loading all skills and crafting our own dictionary of skills

DF_jobs <- read.csv("Model/test.csv")

#making a vector with all the skill names

All_skills <- paste(c(DF_jobs$required_skills), collapse = ", ") %>% 
  scan(text = .,
       what = "character",
       quote = "", 
       sep = ",") %>% 
  trimws(., which = "both") 



Freq_skills <- table(All_skills)%>% 
  sort(decreasing = T) %>% 
  as.data.frame() %>% 
  filter(Freq > 4)


Skill_dictionary <- read.csv("Skill_dictionary.csv")

Skill_dictionary <- as.vector(Skill_dictionary$x)

```


```{r}


#checking which skills are present in the dataset.

skill_check <- cbind(DF_edx, sapply(Skill_dictionary, function(w){grepl(paste0("\\b",w,"\\b"),DF_edx$course_description)}))

# filtering out the skills on the edx DF

DF_edx$Skills <-apply(
  skill_check[,5:ncol(skill_check)], 1, 
  function(u) paste( names(which(u)), collapse=", " ) 
)

#getting the average time it takes to learn a skill (based on the average time of the courses that said skill appeared in)


Skill_times <- tibble( Skill = colnames(skill_check[,5:ncol(skill_check)]), Skill_time = colSums( skill_check[,5:ncol(skill_check)]*skill_check$expected_hours )/colSums(skill_check[,5:ncol(skill_check)])) 


```



```{r}
#saving the csv files. 


write.csv(DF_edx,"Cleaned_edx.csv", row.names = F)

write.csv(Skill_times,"Skill_times.csv", row.names = F)



```



----

### 3. Getting the industry and company size average skill: 

----


```{r}

# Cleaning the glassdoor data----

DF_glassdoor <- read.csv("glassdoor_job_posting.csv") %>% 
  select(Job.Title, Job.Description , Size , Sector)  %>% 
  mutate(Sector = as.factor(Sector))  %>% 
  filter(Size != "-1", Size != "Unknown")

DF_glassdoor$Sector <- DF_glassdoor$Sector  %>%
  gsub("-1", "Data Analysis", .)

# pre-processing the description

DF_glassdoor$Job.Description <- DF_glassdoor$Job.Description %>% 
  tolower() %>%
  gsub("[^[:alnum:]|\\+]", " ", .) %>% 
  gsub('\\b\\w{21,}\\s','', .) %>% 
  removeWords(., stopwords(kind = "en")) %>% 
  gsub("[[:space:]]+", " ", .)

# Checking which skills are present in each description

skill_check_gd <- cbind(DF_glassdoor,
                        sapply(Skill_dictionary, function(w){grepl(paste0("\\b",w,"\\b"),DF_glassdoor$Job.Description)}))

# Adding the skill matches in the text

DF_glassdoor$Skills <-apply(
  skill_check_gd[,5:ncol(skill_check_gd)], 1, 
  function(u) paste( names(which(u)), collapse=", " ) 
)

DF_glassdoor$Size <- DF_glassdoor$Size %>% 
  gsub("1 to 50 Employees|51 to 200 Employees", "Small", .) %>% 
  gsub("201 to 500 Employees|501 to 1000 Employees", "Medium Small", .)%>% 
  gsub("1001 to 5000 Employees|5001 to 10000 Employees", "Medium Large", .)%>% 
  gsub("10000\\+ Employees", "Large", .)

DF_Sector_skills <-   as.tibble(DF_glassdoor) %>% 
  select( Sector, Size, Skills) %>% 
  mutate(Sector = as.factor(Sector), Size = as.factor(Size) ) %>%
  group_by(Sector, Size) %>% 
  mutate(All_skills = paste0(Skills, collapse = ", ")) %>% 
  ungroup() %>% 
  group_by(Sector, Size,All_skills ) %>% 
  summarise()



```



```{r}

#saving the data as .csv


write.csv(DF_glassdoor,"Cleaned_Glassdoor.csv", row.names = F)

write.csv(DF_Sector_skills,"Sector_skills.csv", row.names = F)


```






----

### 4. Making wordclouds

----

```{r,fig.height=3,fig.width=3,warning=FALSE,message=FALSE}

# # Making a loop to print out all word clouds.
# 
# require(wordcloud)
# 
# 
# # Make plots.
# 

i = 7

for (i in 1:28) {


# First: tokenize the data

text_tokens <- scan(text = DF_Sector_skills[[i,3]],
                      what = "character",
                      quote = "",
                    sep = ",")

# Second: make frequency tables

freq_text <- table(text_tokens)%>%
  sort(decreasing = T) %>%
  as.data.frame()

# Third: make wordcloud

pdf(paste0("WC_",DF_Sector_skills[[i,1]],"-", DF_Sector_skills[[i,2]],".pdf"))

layout(matrix(c(1, 2), nrow=2), heights=c(1, 4))
par(mar=rep(0, 4))
plot.new()
text(x=0.5, y=0.5, paste0(DF_Sector_skills[[i,1]]," - ", DF_Sector_skills[[i,2]]))
w_cloud <- wordcloud(words = freq_text$text_tokens,
          freq = freq_text$Freq,
          min.freq = 1)

dev.off()
}


```


----

### 5. Downloading the google forms and transforming it into a useful Dataframe

----





```{r}
# 

DF1 <- read.csv("company2.csv")

DF2 <- data.frame(employee = c("Emely Schutz"," Carly Gerke"," Rosamaria Ledford", "Jina Aylor", "Dione Parenteau", "Ezequiel Crusoe", "Arnoldo Gary", "Stephine Valasquez", "Jazmin Faught", "Gudrun Mcmurtrie"),
                  position = c("database administrator", "application developer", "business intelligence analyst", "network engineer", "project manager", "java developer", "information security analyst", "security architect", "cloud engineer", "quality assurance engineer"),
                  skills = c(" python, databases,excel, sql, machine learning
",
                             " python, excel,ai,  systems, software design, machine learning
",
                             " python, management, r, excel,ai",
                             " python,  databases, excel, sql, 	
 engineering, machine learning
",
                             " python, management, r, excel, business intelligence",
                             " python,java, excel,",
                             " python,  databases, excel,ai, machine learning",
                             " python,  databases, excel, sql",
                             " python,  databases, excel, machine learning
",
                             " python, management, r, excel, sql"))


DF3 <- data.frame(employee = c(),
                  skills = c(),
                  position = c())



write.csv(DF1,"Software_Giant.csv", row.names = F)

write.csv(DF1,"HelloNow.csv", row.names = F)

write.csv(DF1,"Equilibrium.csv", row.names = F)

```













