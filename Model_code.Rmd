---
title: "Tech Skillytics"
author: "T(AI)lents"
date: "20/12/2020"
output:
  pdf_document:
    dev: png
  keep_tex: yes
  word_document: default
header-includes:
- \usepackage{dcolumn}
- \usepackage{booktabs}
- \usepackage{sectsty} \sectionfont{\centering}
- \usepackage{indentfirst}
- \usepackage{setspace}\onehalfspacing
fontsize: 11pt


---


```{r setup, include=FALSE, message=FALSE}

knitr::opts_chunk$set(echo = T, cache = TRUE)
options(scipen = 99999) # Do not wish to have scientific  notations.


```


```{r, warning = F, message = F}
# loading the required packages:

library(FactoMineR)
library(tibble)
library(tm)
library(tidyverse)
library(parallel)
library(ggplot2)
library(tidytext)
require(sentimentr)
library(SentimentAnalysis)
library(lexicon)
require(tokenizers)
require(stringr)

```



\pagebreak

\tableofcontents

\pagebreak


----

### 1. Cleaning up the edx data: 

----

```{r}

# Cleaning the edx data----

DF_edx <- read.csv("edx_courses.csv") %>% 
  select(title, course_description , course_length , course_effort) %>% 
  rename( length_in_weeks = course_length )

DF_edx$course_description <- DF_edx$course_description %>% 
  tolower() %>%
  gsub("[^[:alnum:]|\\+]", " ", .) %>% 
  gsub('\\b\\w{21,}\\s','', .) %>% 
  removeWords(., stopwords(kind = "en")) %>% 
  gsub("[[:space:]]+", " ", .)

DF_edx$title <- DF_edx$title %>% 
  tolower() %>%
  gsub("[^[:alnum:]|\\+]", " ", .) %>% 
  gsub('\\b\\w{21,}\\s','', .) %>% 
  removeWords(., stopwords(kind = "en")) %>% 
  gsub("[[:space:]]+", " ", .)

DF_edx$length_in_weeks <- DF_edx$length_in_weeks  %>% 
  gsub(" Weeks", "", .) %>% 
  as.numeric()

DF_edx <- DF_edx %>% 
  mutate(min_hours_per_week = as.numeric(gsub("â€“.+", "", course_effort)),
         max_hours_per_week = as.numeric(gsub(".+â€“| hours per week", "", course_effort)), 
         avg_hours = (min_hours_per_week +max_hours_per_week)/2,
         expected_hours = avg_hours*length_in_weeks) %>%
  select(title, course_description , expected_hours)



```


----

### 2. Checking the skills present in the edx data

----


```{r}

# loading all skills and crafting our own dictionary of skills

DF_jobs <- read.csv("Model/test.csv")

#making a vector with all the skill names

All_skills <- paste(c(DF_jobs$required_skills), collapse = ", ") %>% 
  scan(text = .,
       what = "character",
       quote = "", 
       sep = ",") %>% 
  trimws(., which = "both") 



Freq_skills <- table(All_skills)%>% 
  sort(decreasing = T) %>% 
  as.data.frame() %>% 
  filter(Freq > 4)

Skill_dictionary <- as.vector(as.character(Freq_skills$All_skills)) %>% 
  gsub("\\+", "\\\\+", .)



```


```{r}



#checking which skills are present in the dataset.

skill_check <- cbind(DF_edx, sapply(Skill_dictionary, function(w){grepl(paste0("\\b",w,"\\b"),DF_edx$course_description)}))

# filtering out the skills on the edx DF

DF_edx$Skills <-apply(
  skill_check[,5:1194], 1, 
  function(u) paste( names(which(u)), collapse=", " ) 
)

#getting the average time it takes to learn a skill (based on the average time of the courses that said skill appeared in)


Skill_times <- tibble( Skill = colnames(skill_check[,5:1194]), Skill_time = colSums( skill_check[,5:1194]*skill_check$expected_hours )/colSums(skill_check[,5:1194])) 


```



```{r}
#saving the csv files. 


write.csv(DF_edx,"Cleaned_edx.csv", row.names = F)

write.csv(Skill_times,"Skill_times.csv", row.names = F)



```



----

### 3. Getting the industry and company size average skill: 

----


```{r}

# Cleaning the glassdoor data----

DF_glassdoor <- read.csv("glassdoor_job_posting.csv") %>% 
  select(Job.Title, Job.Description , Size , Sector)  %>% 
  mutate(Sector = as.factor(Sector))  %>% 
  filter(Size != "-1", Size != "Unknown")

DF_glassdoor$Sector <- DF_glassdoor$Sector  %>%
  gsub("-1", "Data Analysis", .)

# pre-processing the description

DF_glassdoor$Job.Description <- DF_glassdoor$Job.Description %>% 
  tolower() %>%
  gsub("[^[:alnum:]|\\+]", " ", .) %>% 
  gsub('\\b\\w{21,}\\s','', .) %>% 
  removeWords(., stopwords(kind = "en")) %>% 
  gsub("[[:space:]]+", " ", .)

# Checking which skills are present in each description

skill_check_gd <- cbind(DF_glassdoor,
                        sapply(Skill_dictionary, function(w){grepl(paste0("\\b",w,"\\b"),DF_glassdoor$Job.Description)}))

# Adding the skill matches in the text

DF_glassdoor$Skills <-apply(
  skill_check_gd[,5:1195], 1, 
  function(u) paste( names(which(u)), collapse=", " ) 
)

DF_glassdoor$Size <- DF_glassdoor$Size %>% 
  gsub("1 to 50 Employees|51 to 200 Employees", "Small", .) %>% 
  gsub("201 to 500 Employees|501 to 1000 Employees", "Medium Small", .)%>% 
  gsub("1001 to 5000 Employees|5001 to 10000 Employees", "Medium Large", .)%>% 
  gsub("10000\\+ Employees", "Large", .)

DF_Sector_skills <-   as.tibble(DF_glassdoor) %>% 
  select( Sector, Size, Skills) %>% 
  mutate(Sector = as.factor(Sector), Size = as.factor(Size) ) %>%
  group_by(Sector, Size) %>% 
  mutate(All_skills = paste0(Skills, collapse = ", ")) %>% 
  ungroup() %>% 
  group_by(Sector, Size,All_skills ) %>% 
  summarise()



```



```{r}

#saving the data as .csv


write.csv(DF_glassdoor,"Cleaned_Glassdoor.csv", row.names = F)

write.csv(DF_Sector_skills,"Sector_skills.csv", row.names = F)


```






----

### 4. Making wordclouds

----

```{r,fig.height=3,fig.width=3,warning=FALSE,message=FALSE}

# Making a loop to print out all word clouds.

require(wordcloud)



# Make plots.
plot_list = list()
for (i in 1:28) {
    p = ggplot(iris, aes_string(x=var_list[[i]][1], y=var_list[[i]][2])) +
        geom_point(size=3, aes(colour=Species))
    plot_list[[i]] = p
}

# creating a pdf where each page is a separate plot.

pdf("word_clouds.pdf")
for (i in 1:3) {
    print(plot_list[[i]])
}
dev.off()



# First: tokenize the data

text_tokens <- scan(text = DF_Sector_skills[[2,3]],
                      what = "character",
                      quote = "",
                    sep = ",")

# Second: make frequency tables

freq_text <- table(text_tokens)%>% 
  sort(decreasing = T) %>% 
  as.data.frame()

# Third: make wordcloud


jpeg("rplot.jpg", width = 350, height = 350)
layout(matrix(c(1, 2), nrow=2), heights=c(1, 4))
par(mar=rep(0, 4))
plot.new()
text(x=0.5, y=0.5, paste0(DF_Sector_skills[[2,1]], DF_Sector_skills[[2,2]]))
w_cloud <- wordcloud(words = freq_text$text_tokens,
          freq = freq_text$Freq)


```























