---
title: "Tech Skillytics"
author: "T(AI)lents"
date: "20/12/2020"
output:
  pdf_document:
    dev: png
  keep_tex: yes
  word_document: default
header-includes:
- \usepackage{dcolumn}
- \usepackage{booktabs}
- \usepackage{sectsty} \sectionfont{\centering}
- \usepackage{indentfirst}
- \usepackage{setspace}\onehalfspacing
fontsize: 11pt


---


```{r setup, include=FALSE, message=FALSE}

knitr::opts_chunk$set(echo = T, cache = TRUE)
options(scipen = 99999) # Do not wish to have scientific  notations.


```


```{r, warning = F, message = F}
# loading the required packages:

library(FactoMineR)
library(tibble)
library(tm)
library(tidyverse)
library(parallel)
library(ggplot2)
library(tidytext)
require(sentimentr)
library(SentimentAnalysis)
library(lexicon)

### IMPORTANT! ###
# Set the number of cores you will use for parallel calculation.
# Do not exceed the recommended amount. 

CORES <- 8
cl <- parallel::makeCluster(CORES)

```



\pagebreak

\tableofcontents

\pagebreak




### 1. Getting the data to run in R and tidyverse

```{r}

# Cleaning the edx data----

DF_edx <- read.csv("edx_courses.csv") %>% 
  select(title, course_description , course_length , course_effort) %>% 
  rename( length_in_weeks = course_length )

DF_edx$course_description <- DF_edx$course_description %>% 
  tolower() %>%
  gsub("[^[:alnum:]|\\+]", " ", .) %>% 
  gsub('\\b\\w{21,}\\s','', .) %>% 
  removeWords(., stopwords(kind = "en")) %>% 
  gsub("[[:space:]]+", " ", .)

DF_edx$title <- DF_edx$title %>% 
  tolower() %>%
  gsub("[^[:alnum:]|\\+]", " ", .) %>% 
  gsub('\\b\\w{21,}\\s','', .) %>% 
  removeWords(., stopwords(kind = "en")) %>% 
  gsub("[[:space:]]+", " ", .)

DF_edx$length_in_weeks <- DF_edx$length_in_weeks  %>% 
  gsub(" Weeks", "", .) %>% 
  as.numeric()

DF_edx <- DF_edx %>% 
  mutate(min_hours_per_week = as.numeric(gsub("â€“.+", "", course_effort)),
         max_hours_per_week = as.numeric(gsub(".+â€“| hours per week", "", course_effort)), 
         avg_hours = (min_hours_per_week +max_hours_per_week)/2,
         expected_hours = avg_hours*length_in_weeks) %>%
  select(title, course_description , expected_hours)


write.csv(DF_edx,"Cleaned_edx.csv", row.names = F)

DF <- read.csv("Cleaned_edx.csv")

```


### 2. Prepossesing and getting the data ready for modelling. 


\pagebreak

**3.2 Prepossessing the data** 

Now we can preposses the data. In the case of our corpus, we: 

- Removed datapoints with no title, and bodies with less than 30 words.

- Removed uppercase letters.

- Removed any character that isn't alphabetical

- Removed any words that were shorter than 2 letters and larger than 21 letters

- Removed stopwords (words that are frequent but carry no meaning). 


```{r}

# Now we do some prepossessing of the data



```

\pagebreak

**3.3 Tokenizing the data and creating bigrams**

```{r}


```





